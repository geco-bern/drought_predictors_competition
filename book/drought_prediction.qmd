# Drought prediction {#sec-drought-prediction}

Understanding drivers of drought stress in vegetation can help predict stress
in space or under future conditions.

In the below example we use the fractional Light Use Efficiency (fLUE) as a 
proxy for vegetation stress. It describes reductions in the light use efficiency
of plants. These reductions can be due to various stressors such as drought 
(soil or air), nutrient limitations, heat, etc. For simplicity we
assume for our interpretation that the reductions in our data set are linked to
soil drought.

The fLUE data were derived from measurements from eddy covariance sites from the 
[Fluxnet](https://fluxnet.org) network.

Our goal is to build a model that predicts the fractional Light Use Efficiency 
(fLUE) using predictors that are widely available through satellite remote 
sensing or climate reanalysis (ERA5Land) data sets.

We build our model with a training data set, that contains 70 sites. 
Cross-validation 
Eventually, the model is then evaluated on the test data set. Note that this 
exercise is part of a amicable competition so we withheld the true target values
in the test data set. Please make your predictions for the test data set using
your best performing model and submit them to the leaderboard webpage as instructed
in the last code block below (under tab 'For Exercises').

## Data exploration

We start by loading the necessary libraries and the pre-processed data set.

```{r load libraries, include = TRUE}
#| warning: false

# random forest with leave-group of sites-out CV, 
#                  no initial split, 
#                  comprehensive hp tuning

# load the ecosystem
library(caret)
library(ranger)
library(rlang)  # ← required for sym()
library(purrr)
library(dplyr)
library(tidyr)
library(here)
library(ggplot2)
library(readr)
library(tictoc)
library(recipes)
library(vip)
library(themis)
# remotes::install_github("geco-bern/rgeco")
library(rgeco)

# for reproducibilty
set.seed(0)

## Read data --------------------------------------------------------
df_raw <- read_rds(here("../data/competition2025_training_data.rds"))
```

We have a first look at our target variable fLUE. We consider values below 1 to
be drought related and color them red. Note how the different sites in our data 
set show different ranges and regularities of low fLUE values. This is likely
related to different climates and different vegetation types at these sites.

```{r plot targets, include = TRUE}
#| fig-height: 8
#| fig-width: 7.2
ggplot(drop_na(df_raw, is_flue_drought), 
       aes(x=date, y=flue, 
           color = is_flue_drought, 
           group = site)) +
  geom_line() +
  scale_colour_manual("Drought", 
                      values = c("TRUE"="red","FALSE"="black")) +
  facet_wrap(~site) +
  theme_classic() +
  scale_x_date("date", date_labels = "%Y", 
               breaks = as.Date(c("2005-01-01", "2015-01-01")))#date_breaks = "10 years")
```


Our model should learn about these different conditions by the predictor 
variables that we provide. If we look at the data set `df_raw` we find the
following variables present:
```{r show test data, include = TRUE}
# TARGETS: ======
# flue : fractional light use efficiency, a measure of 
#        (drought) stress (our target variable)

# META DATA: ======
# site : Fluxnet site name
# date : date
# is_flue_drought : a flag for drought conditions based on fLUE 
#                   (not to be used as predictor)

# PREDICTORS: ======
# NR_B1 : MODIS normalised reflectance band 1, 620–670nm (-)
# NR_B2 : MODIS normalised reflectance band 2, 841–876nm (-)
# NR_B3 : MODIS normalised reflectance band 3, 459–479nm (-)
# NR_B4 : MODIS normalised reflectance band 4, 545–565nm (-)
# NR_B5 : MODIS normalised reflectance band 5, 1230–1250nm (-)
# NR_B6 : MODIS normalised reflectance band 6, 1628–1652nm (-)
# NR_B7 : MODIS normalised reflectance band 7, 2105–2155nm (-)
# LST : Land surface temperature (Kelvin)
# t2m_era5 : 2-m air temperature (Kelvin)
# ssrd_era5 : Surface solar radiation downwards (W/m2)
# pcwd_era5 : Potential cumulative water deficit (mm)
# vegtype : Vegetation type

df_raw
```

We check for missing values in our data set:

```{r plot missingness, include = TRUE}
visdat::vis_miss(df_raw, 
                 warn_large_data = F)
```

Note that there are some missing values in the target and in the predictor 
values as illustrated above.
The predictor values could potentially be imputed, but care has to be taken her 
to avoid leaking data from the validation sets into the training sets.
For simplicity, we simply drop any rows containing missing values in any of the
target or predictor values with the function `drop_na()`.

```{r drop missing values, include = TRUE}
df <- df_raw |>
  # not needed
  select(-cluster, -cluster_vegtype) |>
  # correct
  mutate(is_flue_drought = as.factor(is_flue_drought)) |>
  # drop rows with missing data is needed variables
  drop_na("flue", "is_flue_drought", starts_with("NR_"), "LST", ends_with("_era5"), "vegtype")
```

Note the imbalance in the data set with respect to (drougth) stressed conditions.

```{r count number of data points, include = TRUE}
df |>
  group_by(is_flue_drought) |> summarise(Number_of_observations = n())
```

## Model training

With the data.frame `df` containing our training data we can thus start the model
training.

As a first step, we want to create additional features by considering 
normalised difference indices between each band combinations,
e.g. when applied to near-infrared and red this results in the 
widely used vegetation stress index 
[NDVI](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index).
We define these features by generating for each band combination an expression
`nd_exprs` and add these features in our recipe with the step `step_mutate`.

In our recipe we include other model preprocessing steps such as normalization 
and numeric encoding of vegetation types. Moreover, in our training data set 
only about a third of all values show stressed conditions (fLUE < 1). Since our 
focus is on predicting these drops, we artificially increase their presence to 
make the model evaluation criteria more sensitive to correctly predicting these 
stressed conditions.

```{r setup the training recipe, include = TRUE}
## Common training setup -------------------------------------------------------
# construct normalised difference indices
band_names <- df |>
  select(starts_with("NR_B")) |>
  names()
band_pairs <- combn(band_names, 2, simplify = FALSE)

# Build named list of expressions using set_names()
nd_names <- band_pairs |>
  map_chr(~ paste0("nd_", .x[1], "_", .x[2]))
nd_exprs <- band_pairs |>
  map(~ expr((!!sym(.x[1]) - !!sym(.x[2])) / (!!sym(.x[1]) + !!sym(.x[2]) + 1e-8)))
nd_exprs <- set_names(nd_exprs, nd_names)


# Define recipe
rec <- recipe(
  flue ~ .,
  data = df
) |>

  # Role handling
  update_role(site, date, is_flue_drought, new_role = "ID") |>
  update_role_requirements("ID", bake = FALSE) |>
  # Feature engineering
  step_mutate(!!!nd_exprs) |>

  # Preprocessing (only model-predictors!)
  step_normalize(all_numeric_predictors()) |>
  step_novel() |>
  step_dummy(vegtype) |>

  # upsample cases when flue is < 1. flue-droughts are now overemphasised and
  # make up (over_ratio) times the cases of cases for which is_flue_drought is
  # false.
  step_upsample(is_flue_drought, over_ratio = 1) |>
  step_rm(is_flue_drought)

# check roles
summary(rec)
```

```{r setup other training settings, include = TRUE}
# Cross-validation by site (number of folds corresponds to number of sites) or group of sites
set.seed(0)
folds <- caret::groupKFold(
  df$site,
  k = length(unique(df$site))
  )
traincntrlParams <- caret::trainControl(
  index = folds,
  method = "cv",
  savePredictions = "final"   # predictions on each validation resample are then available as modl$pred$Resample
)

tune_grid <- expand.grid(
  .mtry = 7, # c(3, 5, 7),
  .min.node.size = 15, # c(5, 15, 25),
  .splitrule = "variance"
)

```

With the training recipe and other settings (`fold`, `traincntrlParams`, 
`tune_grid`) set up, we can run the model training with the above specified 
41 folds.
TODO: Specify why we do it this way!
TODO: Specify below: `sample.fraction = 0.5`, `num.trees       = 500`

```{r run the training, include = TRUE, eval = FALSE}
model <- train(
  rec,
  data            = df,
  metric          = "RMSE",
  method          = "ranger",
  tuneGrid        = tune_grid,
  trControl       = traincntrlParams,
  replace         = TRUE,
  sample.fraction = 0.5,
  num.trees       = 500,         # to be boosted to 2000 for the final model
  importance      = "impurity",  # for variable importance analysis, alternative: "permutation"
  num.threads     = 5            # this is passed down to ranger, do not use in combination with carets own parallelism
)
# The above takes about 20 minutes for 42 folds on a recent MacBook Air.
# The combination with the optimal resampling statistic is chosen as the final 
# model and the entire training set is used to fit a final model.
 
saveRDS(model, file = here("../data/model_rf_2025.rds"), compress = "xz")
```

## Model evaluation on validation data

We can now evaluate the goodness of the trained model on the out-of-sample data 
from our cross-validation folds applied to the training data. 
Furthermore, the variable importance plots indicates the relative importance of 
different predictors in the final model. Note how this includes e.g. 
`nd_NR_B1_NR_B4` which is a feature that was derived (engineered) from the 
predictors present in the input data set.

```{r evaluate on the out-of-sample training data, include = TRUE}
#| warning: false

# inspect out-of-sample validation results visually
model <- readRDS(file = here("../data/model_rf_2025.rds"))
preds <- model$pred
preds$site <- df$site[preds$rowIndex]

# evaluate overall skill on pooled data from all sites
out <- rgeco::analyse_modobs2(
  preds,
  mod = "pred",
  obs = "obs",
  type = "hex",
  pal = "magma",
  shortsubtitle = TRUE
)

out$gg

# variable importance plot
vip(model)
```


```{r plot by site out-of-sample training data, include = FALSE}
#| warning: false
#| fig-height: 20
#| fig-width: 20

### NOTE: THIS WAS REMOVED (include = FALSE) as being not relevant for the exercise

### Note that further improvements could be found by checking the pred-vs-obs
### plots for each site separately.

# evaluate by site
my_analyse_modobs2 <- function(df, use_sitename, ...){

  out <- rgeco::analyse_modobs2(
    df,
    mod = "pred",
    obs = "obs",
    type = "hex",
    pal = "magma",
    shortsubtitle = TRUE,
    plot_subtitle = FALSE
  )

  out$gg <- out$gg +
    labs(title = use_sitename)

  return(out)
}

preds_nested <- preds |>
  group_by(site) |>
  nest() |>
  mutate(modobs = purrr::map2(
    data,
    site,
    ~my_analyse_modobs2(.x, .y))) |>
  mutate(gg = purrr::map(modobs, "gg"))

cowplot::plot_grid(
  plotlist = preds_nested$gg[1:20],
  ncol = 4
)
# cowplot::plot_grid(
#   plotlist = preds_nested$gg[21:40],
#   ncol = 4
# )
# cowplot::plot_grid(
#   plotlist = preds_nested$gg[41:60],
#   ncol = 4
# )
# cowplot::plot_grid(
#   plotlist = preds_nested$gg[61:69],
#   ncol = 4
# )
# 
# 


### TODO: discuss what we see above in the by-site evaluation
```



## Model evaluation on test data

Now let's also evaluate our model on the test data. 
First, we load the test data and `predict()` with our trained model.
We can then use again the function `rgeco::analyse_modobs2` to compute model 
skills (RMSE) by comparing our prediction with the true fLUE values on our test 
data set.

For the exercise you will not have access to the true fLUE values. You can 
evaluate your model by committing your predictions to the repository of the 
course webpage of AGDS II as described under the tab 'For Exercises'.

The comparison of the fitted model with the test data below indicates a good 
model fit. The R squared turns out to be better than on the training data 
(0.84 > 0.63) which indicates that the generalisability of the model appears to 
be very good.

```{r Load test data, include = FALSE, eval = TRUE}
#| warning: false
library(stringr)

# check if on github
ON_GIT <- ifelse(
  Sys.getenv("GITHUB_ACTION") == "",
  FALSE,
  TRUE
)

# if on git read the test labels from
# environmental variable (these are the same
# as for the competition!!)
if(ON_GIT) {
  # LC1 <- Sys.getenv("fLUE")
  # LC1 <- as.numeric(unlist(strsplit(LC1, "\r\n")))
  
  # read test data (without labels)
  df_test <- read_rds(here("../data/competition2025_test_data.rds")) |>
    mutate(flue = NA, is_flue_drought = na_lgl) |> # use hidden test labels
    select(site, date, flue, is_flue_drought, everything())
} else {
  df_test <- read_rds(here("../data/competition2025_test_data_complete.rds")) # NOTE: this can't be used for the competition
}

# and predict:
best_model <- readRDS(file = here("../data/model_rf_2025.rds"))
test_results <- predict(best_model, newdata = df_test)
df_test <- df_test |> mutate(flue_pred = test_results)
```

```{r Predict test data, include = TRUE, eval = FALSE}
# run the model on our test data
# using predict()

df_test <- read_rds(here("../data/competition2025_test_data.rds"))
test_results <- predict(model, newdata = df_test)
df_test <- df_test |> mutate(flue_pred = test_results)
```

::: {.panel-tabset}
## For Book
```{r Evaluate test data - book, include = TRUE}
#| warning: false
out <- rgeco::analyse_modobs2(
  df_test |> rename('obs' = 'flue',
                    'pred' = 'flue_pred'),
  # mod = "flue_pred",
  # obs = "flue",
  mod = "pred",
  obs = "obs",
  
  type = "hex",
  pal = "magma",
  shortsubtitle = TRUE
)
out$gg

out$df_metrics
```
## For Exercises
```{r Evaluate test data - exercise, include = TRUE, eval = FALSE}
# NOTE: for the competition, column 'flue' was removed from 
#       the test data. Thus you can NOT compute the scores 
#       and generate a pred vs obs plot yourself.
#       
#       Instead upload your predictions from your best 
#       model as a pull request to
#       https://github.com/geco-bern/agds2_course.
#       
#       To do so: 
#       a) save your results as .*csv: 
           readr::write_csv(
             dplyr::select(df_test, site, date, flue_pred),
             file = "~/Desktop/username_results.csv")
#       a) fork the repository 'agds2_course', 
#       b) add and commit your results as 
#          'data/leaderboard/fLUE_fall_2025/[username]_results.csv', 
#          where you replace [username] with your GitHub username
#       c) and open a pull request (PR) from your forked 
#          repository to the main repository at 
#          'geco-bern/agds2_course'.
#       
#       Your predictions will then be added to the the leader 
#       board on the course website at 
#       https://geco-bern.github.io/agds2_course/leaderboard_fLUE_fall_2025.html
```
:::

